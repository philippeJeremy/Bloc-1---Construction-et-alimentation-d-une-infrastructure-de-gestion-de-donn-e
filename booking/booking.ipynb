{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import scrapy\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperation des données météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes = [\"Le Mont-Saint-Michel\",\"Saint-Malo\",\"Bayeux\",\"Le Havre\",\"Rouen\",\"Paris\",\"Amiens\",\"Lille\",\"Strasbourg\",\"Château du Haut-Kœnigsbourg\",\"Colmar\",\"Eguisheim\",\"Besançon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\"Gorges du Verdon\",\"Bormes-les-Mimosas\",\"Cassis\",\"Marseille\",\"Aix-en-Provence\",\"Avignon\",\"Uzès\",\"Nîmes\",\"Aigues-Mortes\",\"Les Saintes-Maries-de-la-Mer\",\"Collioure\",\"Carcassonne\",\"Ariège\",\"Toulouse\",\"Montauban\",\"Biarritz\",\"Bayonne\",\"La Rochelle\"]\n",
    "key = os.getenv(\"APIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = []\n",
    "for ville in villes:\n",
    "    url = f\"https://nominatim.openstreetmap.org/?addressdetails=1&q={ville}&format=json&limit=1\"\n",
    "    r = requests.get(url)\n",
    "    lat = r.json()[0][\"lat\"]\n",
    "    lon = r.json()[0][\"lon\"]\n",
    "    a = requests.get(f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={key}&units=metric\")\n",
    "    data = a.json()\n",
    "    temps.append({\"ville_rechercher\" : ville, data.get('list')[1].get('dt_txt'): {\"temperature\" : data.get('list')[1].get('main').get('temp_max'),\n",
    "                                                                        \"temps\" : data.get('list')[1].get('weather')[0].get('description')},\n",
    "                                    data.get('list')[9].get('dt_txt'): {\"temperature\" :data.get('list')[9].get('main').get('temp_max'),\n",
    "                                                                        \"temps\" : data.get('list')[9].get('weather')[0].get('description')},\n",
    "                                    data.get('list')[17].get('dt_txt'): {\"temperature\" :data.get('list')[17].get('main').get('temp_max'),\n",
    "                                                                        \"temps\" : data.get('list')[17].get('weather')[0].get('description')},\n",
    "                                    data.get('list')[25].get('dt_txt'): {\"temperature\" :data.get('list')[25].get('main').get('temp_max'),\n",
    "                                                                        \"temps\" : data.get('list')[25].get('weather')[0].get('description')},\n",
    "                                    data.get('list')[33].get('dt_txt'): {\"temperature\" :data.get('list')[33].get('main').get('temp_max'),\n",
    "                                                                        \"temps\" : data.get('list')[33].get('weather')[0].get('description')}}) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temps.json', 'w', newline=\"\") as f:\n",
    "        json.dump(temps, f) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:43:57 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapybot)\n",
      "2022-11-22 20:43:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2022-11-22 20:43:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True, 'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-11-22 20:43:57 [py.warnings] WARNING: c:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\utils\\request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-11-22 20:43:57 [scrapy.extensions.telnet] INFO: Telnet Password: 25dbf93157317b8a\n",
      "2022-11-22 20:43:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-11-22 20:43:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-11-22 20:43:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-11-22 20:43:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-11-22 20:43:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-11-22 20:43:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-11-22 20:43:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-11-22 20:44:07 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:10 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:11 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:12 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:13 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:14 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:15 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:15 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:16 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:17 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:17 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:18 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:18 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:18 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:19 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:19 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:19 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:20 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:20 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:20 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:20 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:21 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:21 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:22 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:22 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:23 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:23 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:24 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:24 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:25 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:25 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:26 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:26 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:27 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:27 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-11-22 20:44:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-11-22 20:44:27 [scrapy.extensions.feedexport] INFO: Stored json feed (875 items) in: hotel_1.json\n",
      "2022-11-22 20:44:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 49417,\n",
      " 'downloader/request_count': 36,\n",
      " 'downloader/request_method_count/GET': 36,\n",
      " 'downloader/response_bytes': 7386760,\n",
      " 'downloader/response_count': 36,\n",
      " 'downloader/response_status_count/200': 36,\n",
      " 'elapsed_time_seconds': 29.246128,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 11, 22, 19, 44, 27, 355935),\n",
      " 'httpcompression/response_bytes': 42453045,\n",
      " 'httpcompression/response_count': 36,\n",
      " 'item_scraped_count': 875,\n",
      " 'log_count/INFO': 46,\n",
      " 'log_count/WARNING': 1,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 36,\n",
      " 'scheduler/dequeued': 36,\n",
      " 'scheduler/dequeued/memory': 36,\n",
      " 'scheduler/enqueued': 36,\n",
      " 'scheduler/enqueued/memory': 36,\n",
      " 'start_time': datetime.datetime(2022, 11, 22, 19, 43, 58, 109807)}\n",
      "2022-11-22 20:44:27 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class QuotesSpider1(scrapy.Spider):\n",
    "    name = \"spider1\"\n",
    "    start_urls = ['https://www.booking.com/index.fr.html']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        villes = [\"Le Mont-Saint-Michel\",\"St Malo\",\"Bayeux\",\"Le Havre\",\"Rouen\",\"Paris\",\"Amiens\",\"Lille\",\"Strasbourg\",\"Chateau du Haut Koenigsbourg\",\"Colmar\",\"Eguisheim\",\"Besancon\",\"Dijon\",\"Annecy\",\"Grenoble\",\"Lyon\",\"Gorges du Verdon\", \n",
    "                \"Bormes les Mimosas\",\"Cassis\",\"Marseille\",\"Aix-en-Provence\",\"Avignon\",\"Uzès\",\"Nimes\",\"Aigues Mortes\",\"Saintes Maries de la mer\",\"Collioure\",\"Carcassonne\",\"Ariege\",\"Toulouse\",\"Montauban\",\"Biarritz\",\"Bayonne\",\"La Rochelle\"]\n",
    "        for x in villes:\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'ss': x },\n",
    "            callback=self.after_search\n",
    "        )\n",
    "    def after_search(self, response):\n",
    "        villes = response.url.split(\"ss=\")[-1].split(\"&\")[0]\n",
    "        keys = response.css('div.a826ba81c4.fe821aea6c.fa2f36ad22.afd256fc79.d08f526e0d.ed11e24d01.ef9845d4b3.da89aeb942')\n",
    "        url_hotels = [] \n",
    "\n",
    "        for key in keys:\n",
    "            recherche = response.css('input.ce45093752::attr(value)').get()\n",
    "            url_hotel = key.css('a.e13098a59f::attr(href)').get()\n",
    "            url_hotels.append(url_hotel)\n",
    "            \n",
    "            yield {\n",
    "                \"ville_rechercher\" : recherche,\n",
    "                \"ville_précise\": key.css(\"span.f4bd0794db.b4273d69aa::text\").get(),\n",
    "                \"hotel\": key.css('div.fcab3ed991.a23c043802::text').get(),\n",
    "                \"url\": key.css('a.e13098a59f::attr(href)').get(),\n",
    "                \"note\": key.css('div.b5cd09854e.d10a6220b4::text').get() \n",
    "            }\n",
    "\n",
    "        try:\n",
    "            next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)\n",
    "                            \n",
    "filename = \"hotel_1.json\"\n",
    "\n",
    "if filename in os.listdir():\n",
    "        os.remove( filename)\n",
    "        \n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\": True\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider1)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:45:56 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapybot)\n",
      "2022-11-22 20:45:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2022-11-22 20:45:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True, 'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-11-22 20:45:56 [py.warnings] WARNING: c:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scrapy\\utils\\request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-11-22 20:45:56 [scrapy.extensions.telnet] INFO: Telnet Password: 196568398ef35a3d\n",
      "2022-11-22 20:45:56 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-11-22 20:45:56 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-11-22 20:45:56 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-11-22 20:45:56 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-11-22 20:45:56 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-11-22 20:45:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-11-22 20:45:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-11-22 20:46:56 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 44 pages/min), scraped 44 items (at 44 items/min)\n",
      "2022-11-22 20:47:56 [scrapy.extensions.logstats] INFO: Crawled 95 pages (at 51 pages/min), scraped 95 items (at 51 items/min)\n",
      "2022-11-22 20:48:56 [scrapy.extensions.logstats] INFO: Crawled 139 pages (at 44 pages/min), scraped 139 items (at 44 items/min)\n",
      "2022-11-22 20:49:56 [scrapy.extensions.logstats] INFO: Crawled 188 pages (at 49 pages/min), scraped 188 items (at 49 items/min)\n",
      "2022-11-22 20:50:56 [scrapy.extensions.logstats] INFO: Crawled 238 pages (at 50 pages/min), scraped 237 items (at 49 items/min)\n",
      "2022-11-22 20:51:56 [scrapy.extensions.logstats] INFO: Crawled 291 pages (at 53 pages/min), scraped 291 items (at 54 items/min)\n",
      "2022-11-22 20:52:56 [scrapy.extensions.logstats] INFO: Crawled 342 pages (at 51 pages/min), scraped 342 items (at 51 items/min)\n",
      "2022-11-22 20:53:56 [scrapy.extensions.logstats] INFO: Crawled 388 pages (at 46 pages/min), scraped 388 items (at 46 items/min)\n",
      "2022-11-22 20:54:56 [scrapy.extensions.logstats] INFO: Crawled 437 pages (at 49 pages/min), scraped 437 items (at 49 items/min)\n",
      "2022-11-22 20:55:56 [scrapy.extensions.logstats] INFO: Crawled 490 pages (at 53 pages/min), scraped 490 items (at 53 items/min)\n",
      "2022-11-22 20:56:56 [scrapy.extensions.logstats] INFO: Crawled 537 pages (at 47 pages/min), scraped 537 items (at 47 items/min)\n",
      "2022-11-22 20:57:56 [scrapy.extensions.logstats] INFO: Crawled 588 pages (at 51 pages/min), scraped 588 items (at 51 items/min)\n",
      "2022-11-22 20:58:56 [scrapy.extensions.logstats] INFO: Crawled 639 pages (at 51 pages/min), scraped 639 items (at 51 items/min)\n",
      "2022-11-22 20:59:56 [scrapy.extensions.logstats] INFO: Crawled 689 pages (at 50 pages/min), scraped 689 items (at 50 items/min)\n",
      "2022-11-22 21:00:56 [scrapy.extensions.logstats] INFO: Crawled 741 pages (at 52 pages/min), scraped 741 items (at 52 items/min)\n",
      "2022-11-22 21:01:56 [scrapy.extensions.logstats] INFO: Crawled 792 pages (at 51 pages/min), scraped 792 items (at 51 items/min)\n",
      "2022-11-22 21:02:56 [scrapy.extensions.logstats] INFO: Crawled 843 pages (at 51 pages/min), scraped 843 items (at 51 items/min)\n",
      "2022-11-22 21:03:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-11-22 21:03:36 [scrapy.extensions.feedexport] INFO: Stored json feed (875 items) in: hotel_2.json\n",
      "2022-11-22 21:03:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1002872,\n",
      " 'downloader/request_count': 875,\n",
      " 'downloader/request_method_count/GET': 875,\n",
      " 'downloader/response_bytes': 260871463,\n",
      " 'downloader/response_count': 875,\n",
      " 'downloader/response_status_count/200': 875,\n",
      " 'elapsed_time_seconds': 1060.359464,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 11, 22, 20, 3, 36, 917436),\n",
      " 'httpcompression/response_bytes': 1129237098,\n",
      " 'httpcompression/response_count': 875,\n",
      " 'item_scraped_count': 875,\n",
      " 'log_count/INFO': 28,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 875,\n",
      " 'scheduler/dequeued': 875,\n",
      " 'scheduler/dequeued/memory': 875,\n",
      " 'scheduler/enqueued': 875,\n",
      " 'scheduler/enqueued/memory': 875,\n",
      " 'start_time': datetime.datetime(2022, 11, 22, 19, 45, 56, 557972)}\n",
      "2022-11-22 21:03:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class QuotesSpider2(scrapy.Spider):\n",
    "    name = \"spider2\"\n",
    "    file = open(\"hotel_1.json\")\n",
    "    file = json.load(file)\n",
    "    liste_urls = [element[\"url\"] for element in file] \n",
    "\n",
    "    start_urls = liste_urls\n",
    "        \n",
    "    def parse(self, response):\n",
    "            \n",
    "            a = response.css('#hotel_address::attr(data-atlas-latlng)').get(),\n",
    "            b = a[0]\n",
    "            c = b.split(\",\")\n",
    "            \n",
    "            yield {\n",
    "                \"hotel\": response.css('h2.pp-header__title::text').get(),\n",
    "                \"description\" : response.css('#property_description_content > p::text').get(),\n",
    "                \"latitutde\": c[0],\n",
    "                \"longitude\": c[1],\n",
    "            }\n",
    "        \n",
    "filename = \"hotel_2.json\"\n",
    "\n",
    "if filename in os.listdir():\n",
    "        os.remove( filename)\n",
    "        \n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\": True\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider2)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_json(r'C:\\Users\\jerem\\Documents\\kayak\\resultat.json')\n",
    "df_2 = pd.read_json(r'C:\\Users\\jerem\\Documents\\kayak\\hotel.json')\n",
    "df_3 = pd.read_json(r'C:\\Users\\jerem\\Documents\\kayak\\temps.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = os.getenv(\"ACCESS_KEY\")\n",
    "access_key_secrete = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "buket_name = os.getenv(\"BUCKETNAME\")\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=access_key, \n",
    "                        aws_secret_access_key=access_key_secrete)\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.create_bucket(Bucket=buket_name, CreateBucketConfiguration={'LocationConstraint': 'eu-west-3'})\n",
    "put_object = bucket.put_object(Key=\"hotel_1.csv\", Body=df_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23e0aedb6d47e040503db7fcf09a0dff3cea72eb7d5d4c2596a602b1504b448c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
